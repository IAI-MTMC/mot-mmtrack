{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hoang/miniconda3/envs/mot-mmtrack-1x/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from mmtrack.models.mot import QDTrack\n",
    "from mmtrack.apis import batch_inference_mot, init_model, inference_mot\n",
    "from mmdet.models import FasterRCNN\n",
    "import cv2\n",
    "from pytorch_grad_cam import EigenCAM\n",
    "from pytorch_grad_cam.utils.image import show_cam_on_image, scale_cam_image\n",
    "\n",
    "from mmtrack.utils import register_all_modules\n",
    "register_all_modules()\n",
    "from mmdet.models import StandardRoIHead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_boxes(boxes, labels, image):\n",
    "    for i, box in enumerate(boxes):\n",
    "        color = (0, 0, 0)\n",
    "        cv2.rectangle(\n",
    "            image,\n",
    "            (int(box[0]), int(box[1])),\n",
    "            (int(box[2]), int(box[3])),\n",
    "            color, 2\n",
    "        )\n",
    "        cv2.putText(image, str(labels[i]), (int(box[0]), int(box[1] - 5)),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.8, color, 2,\n",
    "                    lineType=cv2.LINE_AA)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BoxScoreTarget:\n",
    "    def __init__(self, labels, bounding_boxes, iou_threshold=0.5):\n",
    "        self.labels = labels\n",
    "        self.bounding_boxes = bounding_boxes\n",
    "        self.iou_threshold = iou_threshold\n",
    "\n",
    "    def __call__(self, model_outputs):\n",
    "        output = torch.Tensor([0])\n",
    "        if torch.cuda.is_available():\n",
    "            output = output.cuda()\n",
    "\n",
    "        if len(model_outputs[\"boxes\"]) == 0:\n",
    "            return output\n",
    "\n",
    "        for box, label in zip(self.bounding_boxes, self.labels):\n",
    "            box = torch.Tensor(box[None, :])\n",
    "            if torch.cuda.is_available():\n",
    "                box = box.cuda()\n",
    "\n",
    "            ious = torchvision.ops.box_iou(box, model_outputs[\"boxes\"])\n",
    "            index = ious.argmax()\n",
    "            if ious[0, index] > self.iou_threshold and model_outputs[\"labels\"][index] == label:\n",
    "                score = ious[0, index] + model_outputs[\"scores\"][index]\n",
    "                output = output + score\n",
    "        return output\n",
    "\n",
    "\n",
    "class WrappedModel(torch.nn.Module):\n",
    "    def __init__(self, config, checkpoint):\n",
    "        super().__init__()\n",
    "        self.model = init_model(config, checkpoint, device=\"cpu\")\n",
    "        assert isinstance(self.model, QDTrack)\n",
    "        self.target_layers = self.model.detector.backbone\n",
    "\n",
    "    def forward(self, img: torch.Tensor):\n",
    "        img = img.moveaxis(0, -1).cpu().numpy()\n",
    "        # results = batch_inference_mot(self.model, [img], [0])[0]\n",
    "        results = inference_mot(self.model, img, 0)\n",
    "\n",
    "        return {\n",
    "            \"boxes\": results.pred_track_instances.bboxes,\n",
    "            \"labels\": results.pred_track_instances.labels,\n",
    "            \"scores\": results.pred_track_instances.scores,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "03/19 09:58:18 - mmengine - INFO - load model from: open-mmlab://detectron2/resnet50_caffe\n",
      "03/19 09:58:18 - mmengine - INFO - Loads checkpoint by openmmlab backend from path: open-mmlab://detectron2/resnet50_caffe\n",
      "03/19 09:58:19 - mmengine - WARNING - The model and loaded state dict do not match exactly\n",
      "\n",
      "unexpected key in source state_dict: conv1.bias\n",
      "\n",
      "03/19 09:58:19 - mmengine - INFO - load model from: https://download.openmmlab.com/mmdetection/v2.0/faster_rcnn/faster_rcnn_r50_fpn_1x_coco-person/faster_rcnn_r50_fpn_1x_coco-person_20201216_175929-d022e227.pth\n",
      "03/19 09:58:19 - mmengine - INFO - Loads checkpoint by http backend from path: https://download.openmmlab.com/mmdetection/v2.0/faster_rcnn/faster_rcnn_r50_fpn_1x_coco-person/faster_rcnn_r50_fpn_1x_coco-person_20201216_175929-d022e227.pth\n",
      "Loads checkpoint by local backend from path: ../checkpoints/qdtrack_faster-rcnn_aic.pth\n"
     ]
    }
   ],
   "source": [
    "from mmcv import VideoReader\n",
    "imgs = VideoReader(\"../demo/test1.mp4\")\n",
    "\n",
    "model = WrappedModel(\n",
    "    config=\"../configs/mot/qdtrack/qdtrack_faster-rcnn_r50_fpn_aic.py\",\n",
    "    checkpoint=\"../checkpoints/qdtrack_faster-rcnn_aic.pth\")\n",
    "\n",
    "img_np = imgs[0]\n",
    "img = torch.from_numpy(img_np).moveaxis(-1, 0)\n",
    "data = model(img)\n",
    "targets = [BoxScoreTarget(data[\"labels\"], data[\"boxes\"])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mmdet.models import StandardRoIHead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1280, 96, 160])\n"
     ]
    }
   ],
   "source": [
    "target_layers = [model.model.detector.neck]\n",
    "def reshape_transform(tensor):\n",
    "    target_size = tensor[0].shape[-2:]\n",
    "    outs = []\n",
    "    for feat in tensor:\n",
    "        outs.append(torch.nn.functional.interpolate(torch.abs(feat), target_size, mode='bilinear'))\n",
    "    outs = torch.cat(outs, dim=1)\n",
    "    print(outs.shape)\n",
    "    return outs\n",
    "cam = EigenCAM(model, target_layers, reshape_transform=reshape_transform, use_cuda=False)\n",
    "\n",
    "grayscale_cam = cam(img, targets=targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1080, 1920)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grayscale_cam.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1080, 1920, 3)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cam_img = show_cam_on_image(img_np / 255.0, grayscale_cam[0], use_rgb=False)\n",
    "cam_img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "draw_boxes(data[\"boxes\"], data[\"labels\"], cam_img)\n",
    "cv2.imwrite(\"cam_img.png\", cam_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mot-mmtrack",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
